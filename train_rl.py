#!/usr/bin/env python3
"""
ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ì„ ì‚¬ìš©í•œ RL í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import time
import numpy as np
import torch
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from main import create_rkss_airport
from utils.scenario_loader import generate_random_scenario, load_scenario_from_dict
from sim.simulation import Simulation
from rl.agent import PPOAgent
from utils.logger import debug, set_training_mode

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'NanumGothic', 'AppleGothic']
    plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    
    try:
        fm._rebuild()
    except AttributeError:
        pass
except ImportError:
    pass  # matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë¬´ì‹œ

def train_rl_with_real_simulation(episodes: int = 50, model_path: str = None):
    """ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ì„ ì‚¬ìš©í•œ RL í›ˆë ¨"""
    print("=== ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ RL í›ˆë ¨ ì‹œì‘ ===")
    
    # í›ˆë ¨ ëª¨ë“œ í™œì„±í™”
    set_training_mode(True)
    
    # RL ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    # ìƒíƒœ í¬ê¸° ê³„ì‚°: 1(ì‹œê°„) + 2*3(í™œì£¼ë¡œ) + 24*2(ë‚ ì”¨) + 20*5(ìŠ¤ì¼€ì¤„) + 1(ì´ë²¤íŠ¸) + 4(í†µê³„) = 1 + 6 + 48 + 100 + 1 + 4 = 160
    observation_size = 160
    action_size = 288  # 2ê°œ í™œì£¼ë¡œ Ã— 144ê°œ ì‹œê°„ ì„ íƒ
    rl_agent = PPOAgent(observation_size=observation_size, action_size=action_size)
    
    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    models_dir = "models"
    model_path = None
    
    if os.path.exists(models_dir):
        model_files = [f for f in os.listdir(models_dir) if f.startswith("ppo_best_") and f.endswith(".pth")]
        if model_files:
            latest_model = sorted(model_files)[-1]
            model_path = os.path.join(models_dir, latest_model)
            rl_agent.load_model(model_path)
            print(f"ê¸°ì¡´ PPO ëª¨ë¸ ë¡œë“œ: {model_path}")
    
    # ìµœê³  ì„±ëŠ¥ ì¶”ì 
    best_reward = float('-inf')
    best_model_path = None
    
    # Early stopping ì„¤ì •
    patience = 30
    no_improvement_count = 0
    
    # í›ˆë ¨ í†µê³„
    training_history = []
    
    for episode in range(episodes):
        print(f"\n--- ì—í”¼ì†Œë“œ {episode + 1}/{episodes} ---")
        
        # ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        airport = create_rkss_airport()
        scenario_dict = generate_random_scenario(num_flights=25, num_events=5)
        schedules, landing_flights, events = load_scenario_from_dict(scenario_dict)
        
        # ì‹œë®¬ë ˆì´ì…˜ ìƒì„±
        sim = Simulation(
            airport=airport,
            schedules=schedules,
            landing_flights=landing_flights,
            events=events,
            mode="TRAINING"
        )
        
        # RL ì—ì´ì „íŠ¸ ì„¤ì •
        sim.set_rl_agent(rl_agent)
        sim.set_training_mode(True)
        sim.scheduler.algorithm = "rl"
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘
        start_time = time.time()
        sim.start()
        
        # ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        while sim.running:
            time.sleep(0.01)  # 10ms ëŒ€ê¸°
        
        # ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ
        end_time = time.time()
        simulation_time = end_time - start_time
        
        # ê²°ê³¼ ê³„ì‚°
        final_reward = -sim.get_total_loss()  # ì†ì‹¤ì„ ìŒìˆ˜ ë³´ìƒìœ¼ë¡œ ë³€í™˜
        
        # ì´ ë¹„í–‰ ìˆ˜ ê³„ì‚° (ì™„ë£Œ + ë‚¨ì€ + ì·¨ì†Œ)
        total_flights = len(sim.completed_schedules) + len(sim.schedules) + sim.cancelled_flights
        completion_rate = len(sim.completed_schedules) / total_flights if total_flights > 0 else 0
        
        # í†µê³„ ì •ë³´
        stats = sim.calculate_statistics()
        print("====================")
        print(f"  ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {simulation_time:.2f}ì´ˆ")
        print(f"  ìµœì¢… ë³´ìƒ: {final_reward:.1f}")
        print(f"  ì™„ë£Œìœ¨: {completion_rate:.1%}")
        print(f"  ì´ ì†ì‹¤: {sim.get_total_loss():.1f}")
        print(f"  ì™„ë£Œëœ ë¹„í–‰: {len(sim.completed_schedules)}")
        print(f"  ë‚¨ì€ ë¹„í–‰: {len(sim.schedules)}")
        print(f"  ì·¨ì†Œëœ ë¹„í–‰: {sim.cancelled_flights}")
        print(f"  ì´ ë¹„í–‰: {total_flights}")
        
        # ê¸°ì¡´ ë””ë²„ê¹… í˜•ì‹ìœ¼ë¡œ Loss ì¶œë ¥
        print("====================")
        print(f"TOTAL DELAY LOSS: {sim.total_delay_loss:.1f}")
        print(f"TOTAL SAFETY LOSS: {sim.total_safety_loss:.1f}")
        print(f"TOTAL SIMULTANEOUS OPS LOSS: {sim.total_simultaneous_ops_loss:.1f}")
        print(f"TOTAL RUNWAY OCCUPIED LOSS: {sim.total_runway_occupied_loss:.1f}")
        print("====================")
        
        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
        training_history.append({
            'episode': episode + 1,
            'reward': final_reward,
            'completion_rate': completion_rate,
            'total_loss': sim.get_total_loss(),
            'completed_flights': len(sim.completed_schedules),
            'remaining_flights': len(sim.schedules),
            'total_flights': total_flights,
            'cancelled_flights': sim.cancelled_flights,
            'delay_loss': sim.total_delay_loss,
            'safety_loss': sim.total_safety_loss,
            'simulation_time': simulation_time
        })
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if final_reward > best_reward:
            best_reward = final_reward
            no_improvement_count = 0  # ê°œì„ ë¨
            timestamp = int(time.time())
            best_model_path = f"models/ppo_best_{timestamp}.pth"
            
            # ëª¨ë¸ ì €ì¥
            os.makedirs("models", exist_ok=True)
            rl_agent.save_model(best_model_path)
            print(f"  ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ë³´ìƒ: {best_reward:.1f}")
            print(f"  ëª¨ë¸ ì €ì¥: {best_model_path}")
        else:
            no_improvement_count += 1
        
        # Early stopping ì²´í¬
        if no_improvement_count >= patience:
            print(f"\nğŸ›‘ Early stopping: {patience} ì—í”¼ì†Œë“œ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (episode + 1) % 10 == 0:
            recent_rewards = [h['reward'] for h in training_history[-10:]]
            avg_reward = np.mean(recent_rewards)
            print(f"\nğŸ“Š ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ: {avg_reward:.1f}")
    
    # í›ˆë ¨ ì™„ë£Œ
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ì´ ì—í”¼ì†Œë“œ: {episodes}")
    print(f"ìµœê³  ë³´ìƒ: {best_reward:.1f}")
    print(f"ìµœê³  ëª¨ë¸: {best_model_path}")
    
    # ì „ì²´ í†µê³„
    all_rewards = [h['reward'] for h in training_history]
    avg_reward = np.mean(all_rewards)
    std_reward = np.std(all_rewards)
    
    print(f"í‰ê·  ë³´ìƒ: {avg_reward:.1f} Â± {std_reward:.1f}")
    print(f"ìµœì € ë³´ìƒ: {min(all_rewards):.1f}")
    print(f"ìµœê³  ë³´ìƒ: {max(all_rewards):.1f}")
    
    # í›ˆë ¨ ê²°ê³¼ ê·¸ë˜í”„ ìƒì„±
    if training_history:
        plot_training_results(training_history)
    
    return best_model_path, training_history

def plot_training_results(training_history):
    """í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        
        episodes = [h['episode'] for h in training_history]
        rewards = [h['reward'] for h in training_history]
        total_losses = [h['total_loss'] for h in training_history]
        delay_losses = [h['delay_loss'] for h in training_history]
        safety_losses = [h['safety_loss'] for h in training_history]
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('RL í›ˆë ¨ ê²°ê³¼', fontsize=16)
        
        # ë³´ìƒ ê·¸ë˜í”„
        ax1.plot(episodes, rewards, 'b-', linewidth=2)
        ax1.set_title('ì—í”¼ì†Œë“œë³„ ë³´ìƒ')
        ax1.set_xlabel('ì—í”¼ì†Œë“œ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.grid(True, alpha=0.3)
        
        # ì´ ì†ì‹¤ ê·¸ë˜í”„
        ax2.plot(episodes, total_losses, 'r-', linewidth=2)
        ax2.set_title('ì—í”¼ì†Œë“œë³„ ì´ ì†ì‹¤')
        ax2.set_xlabel('ì—í”¼ì†Œë“œ')
        ax2.set_ylabel('ì´ ì†ì‹¤')
        ax2.grid(True, alpha=0.3)
        
        # ì§€ì—° ì†ì‹¤ ê·¸ë˜í”„
        ax3.plot(episodes, delay_losses, 'g-', linewidth=2)
        ax3.set_title('ì—í”¼ì†Œë“œë³„ ì§€ì—° ì†ì‹¤')
        ax3.set_xlabel('ì—í”¼ì†Œë“œ')
        ax3.set_ylabel('ì§€ì—° ì†ì‹¤')
        ax3.grid(True, alpha=0.3)
        
        # ì•ˆì „ ì†ì‹¤ ê·¸ë˜í”„
        ax4.plot(episodes, safety_losses, 'orange', linewidth=2)
        ax4.set_title('ì—í”¼ì†Œë“œë³„ ì•ˆì „ ì†ì‹¤')
        ax4.set_xlabel('ì—í”¼ì†Œë“œ')
        ax4.set_ylabel('ì•ˆì „ ì†ì‹¤')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ê·¸ë˜í”„ ì €ì¥
        os.makedirs("plots", exist_ok=True)
        timestamp = int(time.time())
        plot_path = f"plots/training_results_{timestamp}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"í›ˆë ¨ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥: {plot_path}")
        
        plt.show()
        
    except ImportError:
        print("matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í›ˆë ¨ ì„¤ì •
    episodes = 150  # í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜
    
    # ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ (ìˆëŠ” ê²½ìš°)
    models_dir = "models"
    model_path = None
    
    if os.path.exists(models_dir):
        model_files = [f for f in os.listdir(models_dir) if f.startswith("ppo_best_") and f.endswith(".pth")]
        if model_files:
            latest_model = sorted(model_files)[-1]
            model_path = os.path.join(models_dir, latest_model)
            print(f"ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}")
    
    # í›ˆë ¨ ì‹¤í–‰
    best_model_path, training_history = train_rl_with_real_simulation(
        episodes=episodes,
        model_path=model_path
    )
    
    print(f"\ní›ˆë ¨ ì™„ë£Œ! ìµœê³  ëª¨ë¸: {best_model_path}")

if __name__ == "__main__":
    main() 